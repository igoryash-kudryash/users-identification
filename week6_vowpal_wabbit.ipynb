{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://habrastorage.org/web/677/8e1/337/6778e1337c3d4b159d7e99df94227cb2.jpg\"/>\n",
    "## –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è \"–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö\"\n",
    "<center>–ê–≤—Ç–æ—Ä –º–∞—Ç–µ—Ä–∏–∞–ª–∞: –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å Mail.Ru Group, —Å—Ç–∞—Ä—à–∏–π –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –§–∞–∫—É–ª—å—Ç–µ—Ç–∞ –ö–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –ù–∞—É–∫ –í–®–≠ [–Æ—Ä–∏–π –ö–∞—à–Ω–∏—Ü–∫–∏–π](https://yorko.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Capstone –ø—Ä–æ–µ–∫—Ç ‚Ññ1 <br> –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–º –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞–º\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "# <center>–ù–µ–¥–µ–ª—è 6.  Vowpal Wabbit\n",
    "\n",
    "–ù–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ –º—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏–º—Å—è —Å –ø–æ–ø—É–ª—è—Ä–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π Vowpal Wabbit –∏ –ø–æ–ø—Ä–æ–±—É–µ–º –µ–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –ø–æ—Å–µ—â–µ–Ω–∏—é —Å–∞–π—Ç–æ–≤.\n",
    "\n",
    "**–ü–ª–∞–Ω 6 –Ω–µ–¥–µ–ª–∏:**\n",
    "- –ß–∞—Å—Ç—å 1. –°—Ç–∞—Ç—å—è –ø–æ Vowpal Wabbit\n",
    "- –ß–∞—Å—Ç—å 2. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Vowpal Wabbit –∫ –¥–∞–Ω–Ω—ã–º –ø–æ –ø–æ—Å–µ—â–µ–Ω–∏—é —Å–∞–π—Ç–æ–≤\n",
    " - 2.1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    " - 2.2. –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    " - 2.3. –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ (Public Leaderboard)\n",
    "\n",
    "**–í —ç—Ç–æ–π —á–∞—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞ –í–∞–º –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–∏ —Å–ª–µ–¥—É—é—â–∏—Ö –ª–µ–∫—Ü–∏–π –∫—É—Ä—Å–∞ \"–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\":**\n",
    "   - [–°—Ç–æ—Ö–∞—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫](https://www.coursera.org/learn/supervised-learning/lecture/xRY50/stokhastichieskii-ghradiientnyi-spusk)\n",
    "   - [–õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏. `sklearn.linear_model`. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è](https://www.coursera.org/learn/supervised-learning/lecture/EBg9t/linieinyie-modieli-sklearn-linear-model-klassifikatsiia)\n",
    "   \n",
    "–¢–∞–∫–∂–µ –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–∞ [–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è](https://github.com/esokolov/ml-course-msu/blob/master/ML15/lecture-notes/Sem08_vw.pdf) –ª–µ–∫—Ç–æ—Ä–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ï–≤–≥–µ–Ω–∏—è –°–æ–∫–æ–ª–æ–≤–∞. –ò, –∫–æ–Ω–µ—á–Ω–æ –∂–µ, [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](https://github.com/JohnLangford/vowpal_wabbit/wiki) Vowpal Wabbit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ\n",
    "1. –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–¥ –≤ —ç—Ç–æ–π —Ç–µ—Ç—Ä–∞–¥–∫–µ \n",
    "2. –ï—Å–ª–∏ –≤—ã –ø—Ä–æ—Ö–æ–¥–∏—Ç–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –Ø–Ω–¥–µ—Å–∞ –∏ –ú–§–¢–ò, –ø–æ—à–ª–∏—Ç–µ —Ñ–∞–π–ª —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º Programming Assignment. <br> –ï—Å–ª–∏ –≤—ã –ø—Ä–æ—Ö–æ–¥–∏—Ç–µ –∫—É—Ä—Å ODS, –≤—ã–±–µ—Ä–∏—Ç–µ –æ—Ç–≤–µ—Ç—ã –≤ [–≤–µ–±-—Ñ–æ—Ä–º–µ](https://docs.google.com/forms/d/1wteunpEhAt_9s-WBwxYphB6XpniXsAZiFSNuFNmvOdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 1. –°—Ç–∞—Ç—å—è –ø—Ä–æ Vowpal Wabbit\n",
    "–ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ [—Å—Ç–∞—Ç—å—é](https://habrahabr.ru/company/ods/blog/326418/) –ø—Ä–æ Vowpal Wabbit –Ω–∞ –•–∞–±—Ä–µ –∏–∑ —Å–µ—Ä–∏–∏ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –∫—É—Ä—Å–∞ OpenDataScience –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é. –ú–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è —ç—Ç–æ–π —Å—Ç–∞—Ç—å–∏ –∑–∞—Ä–æ–¥–∏–ª—Å—è –∏–∑ –Ω–∞—à–µ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏. –°–∫–∞—á–∞–π—Ç–µ [—Ç–µ—Ç—Ä–∞–¥–∫—É](https://github.com/Yorko/mlcourse_open/blob/master/jupyter_russian/topic08_sgd_hashing_vowpal_wabbit/topic8_sgd_hashing_vowpal_wabbit.ipynb), –ø—Ä–∏–ª–∞–≥–∞–µ–º—É—é –∫ —Å—Ç–∞—Ç—å–µ, –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –∫–æ–¥, –∏–∑—É—á–∏—Ç–µ –µ–≥–æ, –ø–æ–º–µ–Ω—è–π—Ç–µ, —Ç–æ–ª—å–∫–æ —Ç–∞–∫ –º–æ–∂–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è —Å Vowpal Wabbit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 2. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Vowpal Wabbit –∫ –¥–∞–Ω–Ω—ã–º –ø–æ –ø–æ—Å–µ—â–µ–Ω–∏—é —Å–∞–π—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–î–∞–ª–µ–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ Vowpal Wabbit –≤ –¥–µ–ª–µ. –ü—Ä–∞–≤–¥–∞, –≤ –∑–∞–¥–∞—á–µ –Ω–∞—à–µ–≥–æ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–µ–±-—Å–µ—Å—Å–∏–π –º—ã —Ä–∞–∑–Ω–∏—Ü—ã –Ω–µ –∑–∞–º–µ—Ç–∏–º ‚Äì –∫–∞–∫ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É, —Ç–∞–∫ –∏ –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã (—Ö–æ—Ç—è –º–æ–∂–µ—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å), –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å—é —Ä–µ–∑–≤–æ—Å—Ç—å VW –≤ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ 400 –∫–ª–∞—Å—Å–æ–≤. –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤—Å–µ —Ç–µ –∂–µ —Å–∞–º—ã–µ, –Ω–æ –≤—ã–¥–µ–ª–µ–Ω–æ 400 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –∏ —Ä–µ—à–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –∏—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏. –°–∫–∞—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ [–æ—Ç—Å—é–¥–∞](https://kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/data) ‚Äì —Ñ–∞–π–ª—ã `train_sessions_400users.csv` –∏ `test_sessions_400users.csv`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/igor/Acer/Users/arxei/Anaconda/CourseraDeepLearning/Course6/UsersIdentification\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–º–µ–Ω—è–π—Ç–µ –Ω–∞ —Å–≤–æ–π –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º\n",
    "PATH_TO_DATA = './kaggle/400users'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ó–∞–≥—Ä—É–∑–∏–º –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏. –ú–æ–∂–µ—Ç–µ –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–µ—Å—Å–∏–∏ –∑–¥–µ—Å—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —á–µ—Ç–∫–æ –æ—Ç–¥–µ–ª–µ–Ω—ã –æ—Ç —Å–µ—Å—Å–∏–π –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_400 = pd.read_csv(os.path.join(PATH_TO_DATA,'train_sessions_400users.csv'), \n",
    "                           index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_400 = pd.read_csv(os.path.join(PATH_TO_DATA,'test_sessions_400users.csv'), \n",
    "                           index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23713</td>\n",
       "      <td>2014-03-24 15:22:40</td>\n",
       "      <td>23720.0</td>\n",
       "      <td>2014-03-24 15:22:48</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:22:48</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:22:54</td>\n",
       "      <td>23720.0</td>\n",
       "      <td>2014-03-24 15:22:54</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-24 15:22:55</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:01</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:03</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:04</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:05</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8726</td>\n",
       "      <td>2014-04-17 14:25:58</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>665.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>8727.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-04-17 14:26:01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014-04-17 14:26:01</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:18</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:47</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:48</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>2014-03-21 10:12:24</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2014-03-21 10:12:36</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:12:54</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:01</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:24</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-21 10:13:36</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:54</td>\n",
       "      <td>309.0</td>\n",
       "      <td>2014-03-21 10:14:01</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:14:06</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:14:24</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359</td>\n",
       "      <td>2013-12-13 09:52:28</td>\n",
       "      <td>925.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1346.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2013-12-13 09:58:19</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2013-12-13 09:58:19</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>2013-11-26 12:35:29</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:35:31</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2013-11-26 12:35:31</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:37:04</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1    site2                time2    site3  \\\n",
       "session_id                                                                      \n",
       "1           23713  2014-03-24 15:22:40  23720.0  2014-03-24 15:22:48  23713.0   \n",
       "2            8726  2014-04-17 14:25:58   8725.0  2014-04-17 14:25:59    665.0   \n",
       "3             303  2014-03-21 10:12:24     19.0  2014-03-21 10:12:36    303.0   \n",
       "4            1359  2013-12-13 09:52:28    925.0  2013-12-13 09:54:34   1240.0   \n",
       "5              11  2013-11-26 12:35:29     85.0  2013-11-26 12:35:31     52.0   \n",
       "\n",
       "                          time3    site4                time4    site5  \\\n",
       "session_id                                                               \n",
       "1           2014-03-24 15:22:48  23713.0  2014-03-24 15:22:54  23720.0   \n",
       "2           2014-04-17 14:25:59   8727.0  2014-04-17 14:25:59     45.0   \n",
       "3           2014-03-21 10:12:54    303.0  2014-03-21 10:13:01    303.0   \n",
       "4           2013-12-13 09:54:34   1360.0  2013-12-13 09:54:34   1344.0   \n",
       "5           2013-11-26 12:35:31     85.0  2013-11-26 12:35:32     11.0   \n",
       "\n",
       "                          time5  ...                time6    site7  \\\n",
       "session_id                       ...                                 \n",
       "1           2014-03-24 15:22:54  ...  2014-03-24 15:22:55  23713.0   \n",
       "2           2014-04-17 14:25:59  ...  2014-04-17 14:26:01     45.0   \n",
       "3           2014-03-21 10:13:24  ...  2014-03-21 10:13:36    303.0   \n",
       "4           2013-12-13 09:54:34  ...  2013-12-13 09:54:34   1346.0   \n",
       "5           2013-11-26 12:35:32  ...  2013-11-26 12:35:32     11.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1           2014-03-24 15:23:01  23713.0  2014-03-24 15:23:03  23713.0   \n",
       "2           2014-04-17 14:26:01   5320.0  2014-04-17 14:26:18   5320.0   \n",
       "3           2014-03-21 10:13:54    309.0  2014-03-21 10:14:01    303.0   \n",
       "4           2013-12-13 09:54:34   1345.0  2013-12-13 09:54:34   1344.0   \n",
       "5           2013-11-26 12:37:03     85.0  2013-11-26 12:37:03     10.0   \n",
       "\n",
       "                          time9   site10               time10 user_id  \n",
       "session_id                                                             \n",
       "1           2014-03-24 15:23:04  23713.0  2014-03-24 15:23:05     653  \n",
       "2           2014-04-17 14:26:47   5320.0  2014-04-17 14:26:48     198  \n",
       "3           2014-03-21 10:14:06    303.0  2014-03-21 10:14:24      34  \n",
       "4           2013-12-13 09:58:19   1345.0  2013-12-13 09:58:19     601  \n",
       "5           2013-11-26 12:37:03     85.0  2013-11-26 12:37:04     273  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_400.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–í–∏–¥–∏–º, —á—Ç–æ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ 182793 —Å–µ—Å—Å–∏–π, –≤ —Ç–µ—Å—Ç–æ–≤–æ–π ‚Äì 46473, –∏ —Å–µ—Å—Å–∏–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç 400 —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((182793, 21), (46473, 20), 400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_400.shape, test_df_400.shape, train_df_400['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vowpal Wabbit –ª—é–±–∏—Ç, —á—Ç–æ–± –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –±—ã–ª–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –æ—Ç 1 –¥–æ K, –≥–¥–µ K ‚Äì —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤ –≤ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ ‚Äì 400). –ü–æ—ç—Ç–æ–º—É –ø—Ä–∏–¥–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–∏—Ç—å `LabelEncoder`, –¥–∞ –µ—â–µ –∏ +1 –ø–æ—Ç–æ–º –¥–æ–±–∞–≤–∏—Ç—å (`LabelEncoder` –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –º–µ—Ç–∫–∏ –≤ –¥–∏–∞–ø–æ–∑–æ–Ω –æ—Ç 0 –¥–æ K-1). –ü–æ—Ç–æ–º –Ω–∞–¥–æ –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–∏—Ç—å –æ–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df_400['user_id']\n",
    "class_encoder = LabelEncoder()\n",
    "y_for_vw = class_encoder.fit_transform(y) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–î–∞–ª–µ–µ –±—É–¥–µ–º —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å VW —Å SGDClassifier –∏ —Å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π. –í—Å–µ–º –º–æ–¥–µ–ª—è–º —ç—Ç–∏–º –Ω—É–∂–Ω–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–ª—è sklearn-–º–æ–¥–µ–ª–µ–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã, –∫–∞–∫ –º—ã —ç—Ç–æ –¥–µ–ª–∞–ª–∏ –≤ 5 —á–∞—Å—Ç–∏:**\n",
    "- –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ –æ–±—É—á–∞—é—â–∏—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "- –≤—ã–±–µ—Ä–∏—Ç–µ —Ç–æ–ª—å–∫–æ —Å–∞–π—Ç—ã (–ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Ç 'site1' –¥–æ 'site10')\n",
    "- –∑–∞–º–µ–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—Å–∫–∏ –Ω–∞ –Ω—É–ª–∏ (—Å–∞–π—Ç—ã —É –Ω–∞—Å –Ω—É–º–µ—Ä–æ–≤–∞–ª–∏—Å—å —Å 0)\n",
    "- –ø–µ—Ä–µ–≤–µ–¥–∏—Ç–µ –≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç `csr_matrix`\n",
    "- —Ä–∞–∑–±–µ–π—Ç–µ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é —á–∞—Å—Ç–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['site' + str(i) for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_400 = train_df_400[sites].fillna(0)\n",
    "train_df_400 = train_df_400.astype(int)\n",
    "test_df_400 = test_df_400[sites].fillna(0)\n",
    "test_df_400 = test_df_400.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csr_matrix(data, session_length=10): \n",
    "    # –ù–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–∏ —Å—Ç—Ä–æ–∫–∏  ùëñ  –∏ —Å—Ç–æ–ª–±—Ü–∞  ùëó  –±—É–¥–µ—Ç —Å—Ç–æ—è—Ç—å —á–∏—Å–ª–æ  ùëõ_ùëñùëó\n",
    "    # ‚Äì c–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–∞–π—Ç  ùëó  –≤—Å—Ç—Ä–µ—Ç–∏–ª—Å—è –≤ —Å–µ—Å—Å–∏–∏ –Ω–æ–º–µ—Ä  ùëñ\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    prematrix = []\n",
    "    vocabulary = {}\n",
    "    for one_session in data:\n",
    "        for site_id in one_session:\n",
    "            index = vocabulary.setdefault(site_id, site_id)\n",
    "            indices.append(index)\n",
    "            prematrix.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    return csr_matrix((prematrix, indices, indptr), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23713</td>\n",
       "      <td>23720</td>\n",
       "      <td>23713</td>\n",
       "      <td>23713</td>\n",
       "      <td>23720</td>\n",
       "      <td>23713</td>\n",
       "      <td>23713</td>\n",
       "      <td>23713</td>\n",
       "      <td>23713</td>\n",
       "      <td>23713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8726</td>\n",
       "      <td>8725</td>\n",
       "      <td>665</td>\n",
       "      <td>8727</td>\n",
       "      <td>45</td>\n",
       "      <td>8725</td>\n",
       "      <td>45</td>\n",
       "      <td>5320</td>\n",
       "      <td>5320</td>\n",
       "      <td>5320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>19</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>309</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359</td>\n",
       "      <td>925</td>\n",
       "      <td>1240</td>\n",
       "      <td>1360</td>\n",
       "      <td>1344</td>\n",
       "      <td>1359</td>\n",
       "      <td>1346</td>\n",
       "      <td>1345</td>\n",
       "      <td>1344</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>52</td>\n",
       "      <td>85</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "1           23713  23720  23713  23713  23720  23713  23713  23713  23713   \n",
       "2            8726   8725    665   8727     45   8725     45   5320   5320   \n",
       "3             303     19    303    303    303    303    303    309    303   \n",
       "4            1359    925   1240   1360   1344   1359   1346   1345   1344   \n",
       "5              11     85     52     85     11     52     11     85     10   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "1            23713  \n",
       "2             5320  \n",
       "3              303  \n",
       "4             1345  \n",
       "5               85  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df_400 = pd.concat([train_df_400, test_df_400], axis=0)\n",
    "train_test_df_400.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 229266 entries, 1 to 46473\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   site1   229266 non-null  int64\n",
      " 1   site2   229266 non-null  int64\n",
      " 2   site3   229266 non-null  int64\n",
      " 3   site4   229266 non-null  int64\n",
      " 4   site5   229266 non-null  int64\n",
      " 5   site6   229266 non-null  int64\n",
      " 6   site7   229266 non-null  int64\n",
      " 7   site8   229266 non-null  int64\n",
      " 8   site9   229266 non-null  int64\n",
      " 9   site10  229266 non-null  int64\n",
      "dtypes: int64(10)\n",
      "memory usage: 19.2 MB\n"
     ]
    }
   ],
   "source": [
    "train_test_df_400.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229266, 36656)\n"
     ]
    }
   ],
   "source": [
    "X_train_test_sparse = make_csr_matrix(np.array(train_test_df_400))[:,1::]\n",
    "print(X_train_test_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = X_train_test_sparse[:train_df_400.shape[0]:,:]\n",
    "X_test_sparse = X_train_test_sparse[train_df_400.shape[0]::,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182793, 36656)\n",
      "(46473, 36656)\n",
      "(182793,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_sparse.shape)\n",
    "print(X_test_sparse.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–í—ã–¥–µ–ª–∏–º –æ–±—É—á–∞—é—â—É—é (70%) –∏ –æ—Ç–ª–æ–∂–µ–Ω–Ω—É—é (30%) —á–∞—Å—Ç–∏ –∏—Å—Ö–æ–¥–Ω–æ–π –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏. –î–∞–Ω–Ω—ã–µ –Ω–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º, —É—á–∏—Ç—ã–≤–∞–µ–º, —á—Ç–æ —Å–µ—Å—Å–∏–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_share = int(.7 * train_df_400.shape[0])\n",
    "train_df_part = train_df_400[sites].iloc[:train_share, :]\n",
    "valid_df = train_df_400[sites].iloc[train_share:, :]\n",
    "X_train_part_sparse = X_train_sparse[:train_share, :]\n",
    "X_valid_sparse = X_train_sparse[train_share:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_part = y[:train_share]\n",
    "y_valid = y[train_share:]\n",
    "y_train_part_for_vw = y_for_vw[:train_share]\n",
    "y_valid_for_vw = y_for_vw[train_share:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54838, 10)\n",
      "(54838,)\n"
     ]
    }
   ],
   "source": [
    "print(valid_df.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é, `arrays_to_vw`, –ø–µ—Ä–µ–≤–æ–¥—è—â—É—é –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç Vowpal Wabbit.**\n",
    "\n",
    "–í—Ö–æ–¥:\n",
    " - X ‚Äì –º–∞—Ç—Ä–∏—Ü–∞ `NumPy` (–æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞)\n",
    " - y (–Ω–µ–æ–±—è–∑.) - –≤–µ–∫—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ (`NumPy`). –ù–µ–æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –ø–æ—Å–∫–æ–ª—å–∫—É —Ç–µ—Å—Ç–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É –±—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —ç—Ç–æ–π –∂–µ —Ñ—É–Ω–∫—Ü–∏–µ–π\n",
    " - train ‚Äì —Ñ–ª–∞–≥, True –≤ —Å–ª—É—á–∞–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, False ‚Äì –≤ —Å–ª—É—á–∞–µ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    " - out_file ‚Äì –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É .vw, –≤ –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∞ –∑–∞–ø–∏—Å—å\n",
    " \n",
    "–î–µ—Ç–∞–ª–∏:\n",
    "- –Ω–∞–¥–æ –ø—Ä–æ–π—Ç–∏—Å—å –ø–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º –º–∞—Ç—Ä–∏—Ü—ã `X` –∏ –∑–∞–ø–∏—Å–∞—Ç—å —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–∏–≤ –≤–ø–µ—Ä–µ–¥ –Ω—É–∂–Ω—É—é –º–µ—Ç–∫—É –∫–ª–∞—Å—Å–∞ –∏–∑ –≤–µ–∫—Ç–æ—Ä–∞ `y` –∏ –∑–Ω–∞–∫-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å `|`\n",
    "- –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –Ω–∞ –º–µ—Å—Ç–µ –º–µ—Ç–æ–∫ —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞ –º–æ–∂–Ω–æ –ø–∏—Å–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ, –¥–æ–ø—É—Å—Ç–∏–º, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays_to_vw(X, y=None, train=True, out_file='tmp.vw'):\n",
    "    X_vw = []\n",
    "    for i in range(len(X)):\n",
    "        y_tmp = (str(y[i]) if train else \"1\") + \" | \"\n",
    "        X_tmp = \" \".join(list(map(str, list(map(int, X[i])))))\n",
    "        X_vw.append(y_tmp + X_tmp)\n",
    "    \n",
    "    with open(out_file, 'w') as vw_file:\n",
    "        for x in X_vw:\n",
    "            vw_file.write(x)\n",
    "            vw_file.write('\\n')\n",
    "    return X_vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–∏–º–µ–Ω–∏—Ç–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∫ —á–∞—Å—Ç–∏ –æ–±—É—á–∞—â–µ–π –≤—ã–±–æ—Ä–∫–∏ `(train_df_part, y_train_part_for_vw)`, –∫ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ `(valid_df, y_valid_for_vw)`, –∫–æ –≤—Å–µ–π –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –∏ –∫–æ –≤—Å–µ–π —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –Ω–∞ –≤—Ö–æ–¥ –Ω–∞—à –º–µ—Ç–æ–¥ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏–º–µ–Ω–Ω–æ –º–∞—Ç—Ä–∏—Ü—ã –∏ –≤–µ–∫—Ç–æ—Ä–∞ `NumPy`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 s, sys: 59.4 ms, total: 2.14 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# –±—É–¥–µ—Ç 4 –≤—ã–∑–æ–≤–∞\n",
    "train_part_vw = arrays_to_vw(np.array(train_df_part), np.array(y_train_part_for_vw), \n",
    "                             train=True, out_file=os.path.join(PATH_TO_DATA, \"train_part.vw\"))\n",
    "valid_vw = arrays_to_vw(np.array(valid_df), np.array(y_valid_for_vw), \n",
    "                        train=True, out_file=os.path.join(PATH_TO_DATA, \"valid.vw\"))\n",
    "X_train_vw = arrays_to_vw(np.array(train_df_400), np.array(y_for_vw),\n",
    "                          train=True, out_file=os.path.join(PATH_TO_DATA, \"train.vw\"))\n",
    "X_test_vw = arrays_to_vw(np.array(test_df_400), \n",
    "                         train=False, out_file=os.path.join(PATH_TO_DATA, \"test.vw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–ª–∂–µ–Ω –ø–æ–ª—É—á–∏—Ç—å—Å—è —Ç–∞–∫–∏–º (–æ–Ω –ø–æ–ª—É—á–∏–ª—Å—è).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 | 23713 23720 23713 23713 23720 23713 23713 23713 23713 23713\r\n",
      "82 | 8726 8725 665 8727 45 8725 45 5320 5320 5320\r\n",
      "16 | 303 19 303 303 303 303 303 309 303 303\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 $PATH_TO_DATA/train_part.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 | 7 923 923 923 11 924 7 924 838 7\r\n",
      "160 | 91 198 11 11 302 91 668 311 310 91\r\n",
      "312 | 27085 848 118 118 118 118 11 118 118 118\r\n"
     ]
    }
   ],
   "source": [
    "!head -3  $PATH_TO_DATA/valid.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 | 23713 23720 23713 23713 23720 23713 23713 23713 23713 23713\r\n",
      "82 | 8726 8725 665 8727 45 8725 45 5320 5320 5320\r\n",
      "16 | 303 19 303 303 303 303 303 309 303 303\r\n"
     ]
    }
   ],
   "source": [
    "!head -3  $PATH_TO_DATA/train.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 9 304 308 307 91 308 312 300 305 309\r\n",
      "1 | 838 504 68 11 838 11 838 886 27 305\r\n",
      "1 | 190 192 8 189 191 189 190 2375 192 8\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 $PATH_TO_DATA/test.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å Vowpal Wabbit–Ω–∞ –≤—ã–±–æ—Ä–∫–µ `train_part.vw`. –£–∫–∞–∂–∏—Ç–µ, —á—Ç–æ —Ä–µ—à–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å 400 –∫–ª–∞—Å—Å–∞–º–∏ (`--oaa`), —Å–¥–µ–ª–∞–π—Ç–µ 3 –ø—Ä–æ—Ö–æ–¥–∞ –ø–æ –≤—ã–±–æ—Ä–∫–µ (`--passes`). –ó–∞–¥–∞–π—Ç–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã–π –∫—ç—à-—Ñ–∞–π–ª (`--cache_file`, –º–æ–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ —É–∫–∞–∑–∞—Ç—å —Ñ–ª–∞–≥ `-c`), —Ç–∞–∫ VW –±—É–¥–µ—Ç –±—ã—Å—Ç—Ä–µ–µ –¥–µ–ª–∞—Ç—å –≤—Å–µ —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–æ—Ö–æ–¥—ã –ø–æ –≤—ã–±–æ—Ä–∫–µ (–ø—Ä–æ—à–ª—ã–π –∫—ç—à-—Ñ–∞–π–ª —É–¥–∞–ª—è–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `-k`). –¢–∞–∫–∂–µ —É–∫–∞–∂–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ `b`=26. –≠—Ç–æ —á–∏—Å–ª–æ –±–∏—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è, –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ, —á–µ–º 18 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é. –ù–∞–∫–æ–Ω–µ—Ü, —É–∫–∞–∂–∏—Ç–µ `random_seed`=17. –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∫–∞ –Ω–µ –º–µ–Ω—è–π—Ç–µ, –¥–∞–ª–µ–µ —É–∂–µ –≤ —Å–≤–æ–±–æ–¥–Ω–æ–º —Ä–µ–∂–∏–º–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_vw = os.path.join(PATH_TO_DATA, 'train_part.vw')\n",
    "valid_vw = os.path.join(PATH_TO_DATA, 'valid.vw')\n",
    "train_vw = os.path.join(PATH_TO_DATA, 'train.vw')\n",
    "test_vw = os.path.join(PATH_TO_DATA, 'test.vw')\n",
    "model = os.path.join(PATH_TO_DATA, 'vw_model.vw')\n",
    "pred = os.path.join(PATH_TO_DATA, 'vw_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = \r\n",
      "num sources = 1\r\n",
      "\r\n",
      "VW options:\r\n",
      "  --ring_size arg                       size of example ring\r\n",
      "  --onethread                           Disable parse thread\r\n",
      "\r\n",
      "Update options:\r\n",
      "  -l [ --learning_rate ] arg            Set learning rate\r\n",
      "  --power_t arg                         t power value\r\n",
      "  --decay_learning_rate arg             Set Decay factor for learning_rate \r\n",
      "                                        between passes\r\n",
      "  --initial_t arg                       initial t value\r\n",
      "  --feature_mask arg                    Use existing regressor to determine \r\n",
      "                                        which parameters may be updated.  If no\r\n",
      "                                        initial_regressor given, also used for \r\n",
      "                                        initial weights.\r\n",
      "\r\n",
      "Weight options:\r\n",
      "  -i [ --initial_regressor ] arg        Initial regressor(s)\r\n",
      "  --initial_weight arg                  Set all weights to an initial value of \r\n",
      "                                        arg.\r\n",
      "  --random_weights arg                  make initial weights random\r\n",
      "  --normal_weights arg                  make initial weights normal\r\n",
      "  --truncated_normal_weights arg        make initial weights truncated normal\r\n",
      "  --sparse_weights                      Use a sparse datastructure for weights\r\n",
      "  --input_feature_regularizer arg       Per feature regularization input file\r\n",
      "\r\n",
      "Parallelization options:\r\n",
      "  --span_server arg                     Location of server for setting up \r\n",
      "                                        spanning tree\r\n",
      "  --threads                             Enable multi-threading\r\n",
      "  --unique_id arg (=0)                  unique id used for cluster parallel \r\n",
      "                                        jobs\r\n",
      "  --total arg (=1)                      total number of nodes used in cluster \r\n",
      "                                        parallel job\r\n",
      "  --node arg (=0)                       node number in cluster parallel job\r\n",
      "\r\n",
      "Diagnostic options:\r\n",
      "  --version                             Version information\r\n",
      "  -a [ --audit ]                        print weights of features\r\n",
      "  -P [ --progress ] arg                 Progress update frequency. int: \r\n",
      "                                        additive, float: multiplicative\r\n",
      "  --quiet                               Don't output disgnostics and progress \r\n",
      "                                        updates\r\n",
      "  -h [ --help ]                         Look here: http://hunch.net/~vw/ and \r\n",
      "                                        click on Tutorial.\r\n",
      "\r\n",
      "Random Seed option:\r\n",
      "  --random_seed arg                     seed random number generator\r\n",
      "\r\n",
      "Feature options:\r\n",
      "  --hash arg                            how to hash the features. Available \r\n",
      "                                        options: strings, all\r\n",
      "  --hash_seed arg (=0)                  seed for hash function\r\n",
      "  --ignore arg                          ignore namespaces beginning with \r\n",
      "                                        character <arg>\r\n",
      "  --ignore_linear arg                   ignore namespaces beginning with \r\n",
      "                                        character <arg> for linear terms only\r\n",
      "  --keep arg                            keep namespaces beginning with \r\n",
      "                                        character <arg>\r\n",
      "  --redefine arg                        redefine namespaces beginning with \r\n",
      "                                        characters of string S as namespace N. \r\n",
      "                                        <arg> shall be in form 'N:=S' where := \r\n",
      "                                        is operator. Empty N or S are treated \r\n",
      "                                        as default namespace. Use ':' as a \r\n",
      "                                        wildcard in S.\r\n",
      "  -b [ --bit_precision ] arg            number of bits in the feature table\r\n",
      "  --noconstant                          Don't add a constant feature\r\n",
      "  -C [ --constant ] arg                 Set initial value of constant\r\n",
      "  --ngram arg                           Generate N grams. To generate N grams \r\n",
      "                                        for a single namespace 'foo', arg \r\n",
      "                                        should be fN.\r\n",
      "  --skips arg                           Generate skips in N grams. This in \r\n",
      "                                        conjunction with the ngram tag can be \r\n",
      "                                        used to generate generalized \r\n",
      "                                        n-skip-k-gram. To generate n-skips for \r\n",
      "                                        a single namespace 'foo', arg should be\r\n",
      "                                        fN.\r\n",
      "  --feature_limit arg                   limit to N features. To apply to a \r\n",
      "                                        single namespace 'foo', arg should be \r\n",
      "                                        fN\r\n",
      "  --affix arg                           generate prefixes/suffixes of features;\r\n",
      "                                        argument '+2a,-3b,+1' means generate \r\n",
      "                                        2-char prefixes for namespace a, 3-char\r\n",
      "                                        suffixes for b and 1 char prefixes for \r\n",
      "                                        default namespace\r\n",
      "  --spelling arg                        compute spelling features for a give \r\n",
      "                                        namespace (use '_' for default \r\n",
      "                                        namespace)\r\n",
      "  --dictionary arg                      read a dictionary for additional \r\n",
      "                                        features (arg either 'x:file' or just \r\n",
      "                                        'file')\r\n",
      "  --dictionary_path arg                 look in this directory for \r\n",
      "                                        dictionaries; defaults to current \r\n",
      "                                        directory or env{PATH}\r\n",
      "  --interactions arg                    Create feature interactions of any \r\n",
      "                                        level between namespaces.\r\n",
      "  --permutations                        Use permutations instead of \r\n",
      "                                        combinations for feature interactions \r\n",
      "                                        of same namespace.\r\n",
      "  --leave_duplicate_interactions        Don't remove interactions with \r\n",
      "                                        duplicate combinations of namespaces. \r\n",
      "                                        For ex. this is a duplicate: '-q ab -q \r\n",
      "                                        ba' and a lot more in '-q ::'.\r\n",
      "  -q [ --quadratic ] arg                Create and use quadratic features\r\n",
      "  --q: arg                              : corresponds to a wildcard for all \r\n",
      "                                        printable characters\r\n",
      "  --cubic arg                           Create and use cubic features\r\n",
      "\r\n",
      "Example options:\r\n",
      "  -t [ --testonly ]                     Ignore label information and just test\r\n",
      "  --holdout_off                         no holdout data in multiple passes\r\n",
      "  --holdout_period arg (=10)            holdout period for test only\r\n",
      "  --holdout_after arg                   holdout after n training examples, \r\n",
      "                                        default off (disables holdout_period)\r\n",
      "  --early_terminate arg (=3)            Specify the number of passes tolerated \r\n",
      "                                        when holdout loss doesn't decrease \r\n",
      "                                        before early termination\r\n",
      "  --passes arg                          Number of Training Passes\r\n",
      "  --initial_pass_length arg             initial number of examples per pass\r\n",
      "  --examples arg                        number of examples to parse\r\n",
      "  --min_prediction arg                  Smallest prediction to output\r\n",
      "  --max_prediction arg                  Largest prediction to output\r\n",
      "  --sort_features                       turn this on to disregard order in \r\n",
      "                                        which features have been defined. This \r\n",
      "                                        will lead to smaller cache sizes\r\n",
      "  --loss_function arg (=squared)        Specify the loss function to be used, \r\n",
      "                                        uses squared by default. Currently \r\n",
      "                                        available ones are squared, classic, \r\n",
      "                                        hinge, logistic, quantile and poisson.\r\n",
      "  --quantile_tau arg (=0.5)             Parameter \\tau associated with Quantile\r\n",
      "                                        loss. Defaults to 0.5\r\n",
      "  --l1 arg                              l_1 lambda\r\n",
      "  --l2 arg                              l_2 lambda\r\n",
      "  --no_bias_regularization arg          no bias in regularization\r\n",
      "  --named_labels arg                    use names for labels (multiclass, etc.)\r\n",
      "                                        rather than integers, argument \r\n",
      "                                        specified all possible labels, \r\n",
      "                                        comma-sep, eg \"--named_labels \r\n",
      "                                        Noun,Verb,Adj,Punc\"\r\n",
      "\r\n",
      "Output model:\r\n",
      "  -f [ --final_regressor ] arg          Final regressor\r\n",
      "  --readable_model arg                  Output human-readable final regressor \r\n",
      "                                        with numeric features\r\n",
      "  --invert_hash arg                     Output human-readable final regressor \r\n",
      "                                        with feature names.  Computationally \r\n",
      "                                        expensive.\r\n",
      "  --save_resume                         save extra state so learning can be \r\n",
      "                                        resumed later with new data\r\n",
      "  --preserve_performance_counters       reset performance counters when \r\n",
      "                                        warmstarting\r\n",
      "  --save_per_pass                       Save the model after every pass over \r\n",
      "                                        data\r\n",
      "  --output_feature_regularizer_binary arg\r\n",
      "                                        Per feature regularization output file\r\n",
      "  --output_feature_regularizer_text arg Per feature regularization output file,\r\n",
      "                                        in text\r\n",
      "  --id arg                              User supplied ID embedded into the \r\n",
      "                                        final regressor\r\n",
      "\r\n",
      "Output options:\r\n",
      "  -p [ --predictions ] arg              File to output predictions to\r\n",
      "  -r [ --raw_predictions ] arg          File to output unnormalized predictions\r\n",
      "                                        to\r\n",
      "\r\n",
      "Audit Regressor:\r\n",
      "  --audit_regressor arg                 stores feature names and their \r\n",
      "                                        regressor values. Same dataset must be \r\n",
      "                                        used for both regressor training and \r\n",
      "                                        this mode.\r\n",
      "\r\n",
      "\r\n",
      "Search options:\r\n",
      "  --search arg                          Use learning to search, \r\n",
      "                                        argument=maximum action id or 0 for LDF\r\n",
      "  --search_task arg                     the search task (use \"--search_task \r\n",
      "                                        list\" to get a list of available tasks)\r\n",
      "\r\n",
      "  --search_metatask arg                 the search metatask (use \r\n",
      "                                        \"--search_metatask list\" to get a list \r\n",
      "                                        of available metatasks)\r\n",
      "  --search_interpolation arg            at what level should interpolation \r\n",
      "                                        happen? [*data|policy]\r\n",
      "  --search_rollout arg                  how should rollouts be executed?       \r\n",
      "                                            [policy|oracle|*mix_per_state|mix_p\r\n",
      "                                        er_roll|none]\r\n",
      "  --search_rollin arg                   how should past trajectories be \r\n",
      "                                        generated? [policy|oracle|*mix_per_stat\r\n",
      "                                        e|mix_per_roll]\r\n",
      "  --search_passes_per_policy arg (=1)   number of passes per policy (only valid\r\n",
      "                                        for search_interpolation=policy)\r\n",
      "  --search_beta arg (=0.5)              interpolation rate for policies (only \r\n",
      "                                        valid for search_interpolation=policy)\r\n",
      "  --search_alpha arg (=1.00000001e-10)  annealed beta = 1-(1-alpha)^t (only \r\n",
      "                                        valid for search_interpolation=data)\r\n",
      "  --search_total_nb_policies arg        if we are going to train the policies \r\n",
      "                                        through multiple separate calls to vw, \r\n",
      "                                        we need to specify this parameter and \r\n",
      "                                        tell vw how many policies are \r\n",
      "                                        eventually going to be trained\r\n",
      "  --search_trained_nb_policies arg      the number of trained policies in a \r\n",
      "                                        file\r\n",
      "  --search_allowed_transitions arg      read file of allowed transitions [def: \r\n",
      "                                        all transitions are allowed]\r\n",
      "  --search_subsample_time arg           instead of training at all timesteps, \r\n",
      "                                        use a subset. if value in (0,1), train \r\n",
      "                                        on a random v%. if v>=1, train on \r\n",
      "                                        precisely v steps per example, if \r\n",
      "                                        v<=-1, use active learning\r\n",
      "  --search_neighbor_features arg        copy features from neighboring lines. \r\n",
      "                                        argument looks like: '-1:a,+2' meaning \r\n",
      "                                        copy previous line namespace a and next\r\n",
      "                                        next line from namespace _unnamed_, \r\n",
      "                                        where ',' separates them\r\n",
      "  --search_rollout_num_steps arg        how many calls of \"loss\" before we stop\r\n",
      "                                        really predicting on rollouts and \r\n",
      "                                        switch to oracle (default means \r\n",
      "                                        \"infinite\")\r\n",
      "  --search_history_length arg (=1)      some tasks allow you to specify how \r\n",
      "                                        much history their depend on; specify \r\n",
      "                                        that here\r\n",
      "  --search_no_caching                   turn off the built-in caching ability \r\n",
      "                                        (makes things slower, but technically \r\n",
      "                                        more safe)\r\n",
      "  --search_xv                           train two separate policies, \r\n",
      "                                        alternating prediction/learning\r\n",
      "  --search_perturb_oracle arg (=0)      perturb the oracle on rollin with this \r\n",
      "                                        probability\r\n",
      "  --search_linear_ordering              insist on generating examples in linear\r\n",
      "                                        order (def: hoopla permutation)\r\n",
      "  --search_active_verify arg            verify that active learning is doing \r\n",
      "                                        the right thing (arg = multiplier, \r\n",
      "                                        should be = cost_range * range_c)\r\n",
      "  --search_save_every_k_runs arg        save model every k runs\r\n",
      "\r\n",
      "Experience Replay:\r\n",
      "  --replay_c arg                        use experience replay at a specified \r\n",
      "                                        level [b=classification/regression, \r\n",
      "                                        m=multiclass, c=cost sensitive] with \r\n",
      "                                        specified buffer size\r\n",
      "\r\n",
      "  --replay_c_count arg (=1)             how many times (in expectation) should \r\n",
      "                                        each example be played (default: 1 = \r\n",
      "                                        permuting)\r\n",
      "\r\n",
      "Explore evaluation:\r\n",
      "  --explore_eval                        Evaluate explore_eval adf policies\r\n",
      "\r\n",
      "  --multiplier arg                      Multiplier used to make all rejection \r\n",
      "                                        sample probabilities <= 1\r\n",
      "\r\n",
      "Make Multiclass into Contextual Bandit:\r\n",
      "  --cbify arg                           Convert multiclass on <k> classes into \r\n",
      "                                        a contextual bandit problem\r\n",
      "\r\n",
      "  --cbify_cs                            consume cost-sensitive classification \r\n",
      "                                        examples instead of multiclass\r\n",
      "  --loss0 arg (=0)                      loss for correct label\r\n",
      "  --loss1 arg (=1)                      loss for incorrect label\r\n",
      "\r\n",
      "Contextual Bandit Exploration with Action Dependent Features:\r\n",
      "  --cb_explore_adf                      Online explore-exploit for a contextual\r\n",
      "                                        bandit problem with multiline action \r\n",
      "                                        dependent features\r\n",
      "\r\n",
      "  --first arg                           tau-first exploration\r\n",
      "  --epsilon arg                         epsilon-greedy exploration\r\n",
      "  --bag arg                             bagging-based exploration\r\n",
      "  --cover arg                           Online cover based exploration\r\n",
      "  --psi arg (=1)                        disagreement parameter for cover\r\n",
      "  --nounif                              do not explore uniformly on \r\n",
      "                                        zero-probability actions in cover\r\n",
      "  --softmax                             softmax exploration\r\n",
      "  --regcb                               RegCB-elim exploration\r\n",
      "  --regcbopt                            RegCB optimistic exploration\r\n",
      "  --mellowness arg (=0.100000001)       RegCB mellowness parameter c_0. Default\r\n",
      "                                        0.1\r\n",
      "  --greedify                            always update first policy once in \r\n",
      "                                        bagging\r\n",
      "  --cb_min_cost arg (=0)                lower bound on cost\r\n",
      "  --cb_max_cost arg (=1)                upper bound on cost\r\n",
      "  --first_only                          Only explore the first action in a \r\n",
      "                                        tie-breaking event\r\n",
      "  --lambda arg (=-1)                    parameter for softmax\r\n",
      "\r\n",
      "Contextual Bandit Exploration:\r\n",
      "  --cb_explore arg                      Online explore-exploit for a <k> action\r\n",
      "                                        contextual bandit problem\r\n",
      "\r\n",
      "  --first arg                           tau-first exploration\r\n",
      "  --epsilon arg (=0.0500000007)         epsilon-greedy exploration\r\n",
      "  --bag arg                             bagging-based exploration\r\n",
      "  --cover arg                           Online cover based exploration\r\n",
      "  --psi arg (=1)                        disagreement parameter for cover\r\n",
      "\r\n",
      "Multiworld Testing Options:\r\n",
      "  --multiworld_test arg                 Evaluate features as a policies\r\n",
      "\r\n",
      "  --learn arg                           Do Contextual Bandit learning on <n> \r\n",
      "                                        classes.\r\n",
      "  --exclude_eval                        Discard mwt policy features before \r\n",
      "                                        learning\r\n",
      "\r\n",
      "Contextual Bandit with Action Dependent Features:\r\n",
      "  --cb_adf                              Do Contextual Bandit learning with \r\n",
      "                                        multiline action dependent features.\r\n",
      "\r\n",
      "  --rank_all                            Return actions sorted by score order\r\n",
      "  --no_predict                          Do not do a prediction when training\r\n",
      "  --cb_type arg (=ips)                  contextual bandit method to use in \r\n",
      "                                        {ips,dm,dr, mtr}\r\n",
      "\r\n",
      "Contextual Bandit Options:\r\n",
      "  --cb arg                              Use contextual bandit learning with <k>\r\n",
      "                                        costs\r\n",
      "\r\n",
      "  --cb_type arg (=dr)                   contextual bandit method to use in \r\n",
      "                                        {ips,dm,dr}\r\n",
      "  --eval                                Evaluate a policy rather than \r\n",
      "                                        optimizing.\r\n",
      "\r\n",
      "Cost Sensitive One Against All with Label Dependent Features:\r\n",
      "  --csoaa_ldf arg                       Use one-against-all multiclass learning\r\n",
      "                                        with label dependent features.\r\n",
      "\r\n",
      "  --ldf_override arg                    Override singleline or multiline from \r\n",
      "                                        csoaa_ldf or wap_ldf, eg if stored in \r\n",
      "                                        file\r\n",
      "  --csoaa_rank                          Return actions sorted by score order\r\n",
      "  --probabilities                       predict probabilites of all classes\r\n",
      "\r\n",
      "  --wap_ldf arg                         Use weighted all-pairs multiclass \r\n",
      "                                        learning with label dependent features.\r\n",
      "                                          Specify singleline or multiline.\r\n",
      "\r\n",
      "\r\n",
      "Interact via elementwise multiplication:\r\n",
      "  --interact arg                        Put weights on feature products from \r\n",
      "                                        namespaces <n1> and <n2>\r\n",
      "\r\n",
      "\r\n",
      "Cost Sensitive One Against All:\r\n",
      "  --csoaa arg                           One-against-all multiclass with <k> \r\n",
      "                                        costs\r\n",
      "\r\n",
      "\r\n",
      "Cost-sensitive Active Learning:\r\n",
      "  --cs_active arg                       Cost-sensitive active learning with <k>\r\n",
      "                                        costs\r\n",
      "\r\n",
      "  --simulation                          cost-sensitive active learning \r\n",
      "                                        simulation mode\r\n",
      "  --baseline                            cost-sensitive active learning baseline\r\n",
      "  --domination                          cost-sensitive active learning use \r\n",
      "                                        domination. Default 1\r\n",
      "  --mellowness arg (=0.100000001)       mellowness parameter c_0. Default 0.1.\r\n",
      "  --range_c arg (=0.5)                  parameter controlling the threshold for\r\n",
      "                                        per-label cost uncertainty. Default \r\n",
      "                                        0.5.\r\n",
      "  --max_labels arg (=18446744073709551615)\r\n",
      "                                        maximum number of label queries.\r\n",
      "  --min_labels arg (=18446744073709551615)\r\n",
      "                                        minimum number of label queries.\r\n",
      "  --cost_max arg (=1)                   cost upper bound. Default 1.\r\n",
      "  --cost_min arg (=0)                   cost lower bound. Default 0.\r\n",
      "  --csa_debug                           print debug stuff for cs_active\r\n",
      "\r\n",
      "Multilabel One Against All:\r\n",
      "  --multilabel_oaa arg                  One-against-all multilabel with <k> \r\n",
      "                                        labels\r\n",
      "\r\n",
      "\r\n",
      "importance weight classes:\r\n",
      "  --classweight arg                     importance weight multiplier for class\r\n",
      "\r\n",
      "\r\n",
      "Recall Tree:\r\n",
      "  --recall_tree arg                     Use online tree for multiclass\r\n",
      "\r\n",
      "  --max_candidates arg                  maximum number of labels per leaf in \r\n",
      "                                        the tree\r\n",
      "  --bern_hyper arg (=1)                 recall tree depth penalty\r\n",
      "  --max_depth arg                       maximum depth of the tree, default \r\n",
      "                                        log_2 (#classes)\r\n",
      "  --node_only arg (=0)                  only use node features, not full path \r\n",
      "                                        features\r\n",
      "  --randomized_routing arg (=0)         randomized routing\r\n",
      "\r\n",
      "Logarithmic Time Multiclass Tree:\r\n",
      "  --log_multi arg                       Use online tree for multiclass\r\n",
      "\r\n",
      "  --no_progress                         disable progressive validation\r\n",
      "  --swap_resistance arg (=4)            higher = more resistance to swap, \r\n",
      "                                        default=4\r\n",
      "\r\n",
      "Error Correcting Tournament Options:\r\n",
      "  --ect arg                             Error correcting tournament with <k> \r\n",
      "                                        labels\r\n",
      "\r\n",
      "  --error arg (=0)                      errors allowed by ECT\r\n",
      "\r\n",
      "Boosting:\r\n",
      "  --boosting arg                        Online boosting with <N> weak learners\r\n",
      "\r\n",
      "  --gamma arg (=0.100000001)            weak learner's edge (=0.1), used only \r\n",
      "                                        by online BBM\r\n",
      "  --alg arg (=BBM)                      specify the boosting algorithm: BBM \r\n",
      "                                        (default), logistic (AdaBoost.OL.W), \r\n",
      "                                        adaptive (AdaBoost.OL)\r\n",
      "\r\n",
      "One Against All Options:\r\n",
      "  --oaa arg                             One-against-all multiclass with <k> \r\n",
      "                                        labels\r\n",
      "\r\n",
      "  --oaa_subsample arg                   subsample this number of negative \r\n",
      "                                        examples when learning\r\n",
      "  --probabilities                       predict probabilites of all classes\r\n",
      "  --scores                              output raw scores per class\r\n",
      "\r\n",
      "Top K:\r\n",
      "  --top arg                             top k recommendation\r\n",
      "\r\n",
      "\r\n",
      "Experience Replay:\r\n",
      "  --replay_m arg                        use experience replay at a specified \r\n",
      "                                        level [b=classification/regression, \r\n",
      "                                        m=multiclass, c=cost sensitive] with \r\n",
      "                                        specified buffer size\r\n",
      "\r\n",
      "  --replay_m_count arg (=1)             how many times (in expectation) should \r\n",
      "                                        each example be played (default: 1 = \r\n",
      "                                        permuting)\r\n",
      "\r\n",
      "Binary loss:\r\n",
      "  --binary                              report loss as binary classification on\r\n",
      "                                        -1,1\r\n",
      "\r\n",
      "\r\n",
      "Bootstrap:\r\n",
      "  --bootstrap arg                       k-way bootstrap by online importance \r\n",
      "                                        resampling\r\n",
      "\r\n",
      "  --bs_type arg                         prediction type {mean,vote}\r\n",
      "\r\n",
      "scorer options:\r\n",
      "  --link arg (=identity)                Specify the link function: identity, \r\n",
      "                                        logistic, glf1 or poisson\r\n",
      "\r\n",
      "Stagewise polynomial options:\r\n",
      "  --stage_poly                          use stagewise polynomial feature \r\n",
      "                                        learning\r\n",
      "\r\n",
      "  --sched_exponent arg (=1)             exponent controlling quantity of \r\n",
      "                                        included features\r\n",
      "  --batch_sz arg (=1000)                multiplier on batch size before \r\n",
      "                                        including more features\r\n",
      "  --batch_sz_no_doubling                batch_sz does not double\r\n",
      "\r\n",
      "Low Rank Quadratics FA:\r\n",
      "  --lrqfa arg                           use low rank quadratic features with \r\n",
      "                                        field aware weights\r\n",
      "\r\n",
      "\r\n",
      "Low Rank Quadratics:\r\n",
      "  --lrq arg                             use low rank quadratic features\r\n",
      "\r\n",
      "  --lrqdropout                          use dropout training for low rank \r\n",
      "                                        quadratic features\r\n",
      "\r\n",
      "Autolink:\r\n",
      "  --autolink arg                        create link function with polynomial d\r\n",
      "\r\n",
      "\r\n",
      "Marginal:\r\n",
      "  --marginal arg                        substitute marginal label estimates for\r\n",
      "                                        ids\r\n",
      "\r\n",
      "  --initial_denominator arg (=1)        initial denominator\r\n",
      "  --initial_numerator arg (=0.5)        initial numerator\r\n",
      "  --compete                             enable competition with marginal \r\n",
      "                                        features\r\n",
      "  --update_before_learn arg (=0)        update marginal values before learning\r\n",
      "  --unweighted_marginals arg (=0)       ignore importance weights when \r\n",
      "                                        computing marginals\r\n",
      "  --decay arg (=0)                      decay multiplier per event (1e-3 for \r\n",
      "                                        example)\r\n",
      "\r\n",
      "Matrix Factorization Reduction:\r\n",
      "  --new_mf arg                          rank for reduction-based matrix \r\n",
      "                                        factorization\r\n",
      "\r\n",
      "\r\n",
      "Neural Network:\r\n",
      "  --nn arg                              Sigmoidal feedforward network with <k> \r\n",
      "                                        hidden units\r\n",
      "\r\n",
      "  --inpass                              Train or test sigmoidal feedforward \r\n",
      "                                        network with input passthrough.\r\n",
      "  --multitask                           Share hidden layer across all reduced \r\n",
      "                                        tasks.\r\n",
      "  --dropout                             Train or test sigmoidal feedforward \r\n",
      "                                        network using dropout.\r\n",
      "  --meanfield                           Train or test sigmoidal feedforward \r\n",
      "                                        network using mean field.\r\n",
      "\r\n",
      "Confidence:\r\n",
      "  --confidence                          Get confidence for binary predictions\r\n",
      "\r\n",
      "  --confidence_after_training           Confidence after training\r\n",
      "\r\n",
      "Active Learning with Cover:\r\n",
      "  --active_cover                        enable active learning with cover\r\n",
      "\r\n",
      "  --mellowness arg (=8)                 active learning mellowness parameter \r\n",
      "                                        c_0. Default 8.\r\n",
      "  --alpha arg (=1)                      active learning variance upper bound \r\n",
      "                                        parameter alpha. Default 1.\r\n",
      "  --beta_scale arg (=3.1622777)         active learning variance upper bound \r\n",
      "                                        parameter beta_scale. Default sqrt(10).\r\n",
      "  --cover arg (=12)                     cover size. Default 12.\r\n",
      "  --oracular                            Use Oracular-CAL style query or not. \r\n",
      "                                        Default false.\r\n",
      "\r\n",
      "Active Learning:\r\n",
      "  --active                              enable active learning\r\n",
      "\r\n",
      "  --simulation                          active learning simulation mode\r\n",
      "  --mellowness arg (=8)                 active learning mellowness parameter \r\n",
      "                                        c_0. Default 8\r\n",
      "\r\n",
      "Experience Replay:\r\n",
      "  --replay_b arg                        use experience replay at a specified \r\n",
      "                                        level [b=classification/regression, \r\n",
      "                                        m=multiclass, c=cost sensitive] with \r\n",
      "                                        specified buffer size\r\n",
      "\r\n",
      "  --replay_b_count arg (=1)             how many times (in expectation) should \r\n",
      "                                        each example be played (default: 1 = \r\n",
      "                                        permuting)\r\n",
      "\r\n",
      "Baseline options:\r\n",
      "  --baseline                            Learn an additive baseline (from \r\n",
      "                                        constant features) and a residual \r\n",
      "                                        separately in regression.\r\n",
      "\r\n",
      "  --lr_multiplier arg                   learning rate multiplier for baseline \r\n",
      "                                        model\r\n",
      "  --global_only                         use separate example with only global \r\n",
      "                                        constant for baseline predictions\r\n",
      "  --check_enabled                       only use baseline when the example \r\n",
      "                                        contains enabled flag\r\n",
      "\r\n",
      "OjaNewton options:\r\n",
      "  --OjaNewton                           Online Newton with Oja's Sketch\r\n",
      "\r\n",
      "  --sketch_size arg (=10)               size of sketch\r\n",
      "  --epoch_size arg (=1)                 size of epoch\r\n",
      "  --alpha arg (=1)                      mutiplicative constant for indentiy\r\n",
      "  --alpha_inverse arg                   one over alpha, similar to learning \r\n",
      "                                        rate\r\n",
      "  --learning_rate_cnt arg (=2)          constant for the learning rate 1/t\r\n",
      "  --normalize arg (=1)                  normalize the features or not\r\n",
      "  --random_init arg (=1)                randomize initialization of Oja or not\r\n",
      "\r\n",
      "LBFGS and Conjugate Gradient options:\r\n",
      "  --conjugate_gradient                  use conjugate gradient based \r\n",
      "                                        optimization\r\n",
      "\r\n",
      "\r\n",
      "  --bfgs                                use bfgs optimization\r\n",
      "\r\n",
      "  --hessian_on                          use second derivative in line search\r\n",
      "  --mem arg (=15)                       memory in bfgs\r\n",
      "  --termination arg (=0.00100000005)    Termination threshold\r\n",
      "\r\n",
      "Latent Dirichlet Allocation:\r\n",
      "  --lda arg                             Run lda with <int> topics\r\n",
      "\r\n",
      "  --lda_alpha arg (=0.100000001)        Prior on sparsity of per-document topic\r\n",
      "                                        weights\r\n",
      "  --lda_rho arg (=0.100000001)          Prior on sparsity of topic \r\n",
      "                                        distributions\r\n",
      "  --lda_D arg (=10000)                  Number of documents\r\n",
      "  --lda_epsilon arg (=0.00100000005)    Loop convergence threshold\r\n",
      "  --minibatch arg (=1)                  Minibatch size, for LDA\r\n",
      "  --math-mode arg (=0)                  Math mode: simd, accuracy, fast-approx\r\n",
      "  --metrics arg (=0)                    Compute metrics\r\n",
      "\r\n",
      "Noop Learner:\r\n",
      "  --noop                                do no learning\r\n",
      "\r\n",
      "\r\n",
      "Print psuedolearner:\r\n",
      "  --print                               print examples\r\n",
      "\r\n",
      "\r\n",
      "Gradient Descent Matrix Factorization:\r\n",
      "  --rank arg                            rank for matrix factorization.\r\n",
      "\r\n",
      "\r\n",
      "Network sending:\r\n",
      "  --sendto arg                          send examples to <host>\r\n",
      "\r\n",
      "\r\n",
      "Stochastic Variance Reduced Gradient:\r\n",
      "  --svrg                                Streaming Stochastic Variance Reduced \r\n",
      "                                        Gradient\r\n",
      "\r\n",
      "  --stage_size arg (=1)                 Number of passes per SVRG stage\r\n",
      "\r\n",
      "Follow the Regularized Leader:\r\n",
      "  --ftrl                                FTRL: Follow the Proximal Regularized \r\n",
      "                                        Leader\r\n",
      "\r\n",
      "  --ftrl_alpha arg (=0.00499999989)     Learning rate for FTRL optimization\r\n",
      "  --ftrl_beta arg (=0.100000001)        FTRL beta parameter\r\n",
      "\r\n",
      "  --pistol                              FTRL: Parameter-free Stochastic \r\n",
      "                                        Learning\r\n",
      "\r\n",
      "  --ftrl_alpha arg (=1)                 Learning rate for FTRL optimization\r\n",
      "  --ftrl_beta arg (=0.5)                FTRL beta parameter\r\n",
      "\r\n",
      "Kernel SVM:\r\n",
      "  --ksvm                                kernel svm\r\n",
      "\r\n",
      "  --reprocess arg (=1)                  number of reprocess steps for LASVM\r\n",
      "  --pool_greedy                         use greedy selection on mini pools\r\n",
      "  --para_active                         do parallel active learning\r\n",
      "  --pool_size arg (=1)                  size of pools for active learning\r\n",
      "  --subsample arg (=1)                  number of items to subsample from the \r\n",
      "                                        pool\r\n",
      "  --kernel arg (=linear)                type of kernel (rbf or linear \r\n",
      "                                        (default))\r\n",
      "  --bandwidth arg (=1)                  bandwidth of rbf kernel\r\n",
      "  --degree arg (=2)                     degree of poly kernel\r\n",
      "  --lambda arg                          saving regularization for test time\r\n",
      "\r\n",
      "Gradient Descent options:\r\n",
      "  --sgd                                 use regular stochastic gradient descent\r\n",
      "                                        update.\r\n",
      "  --adaptive                            use adaptive, individual learning \r\n",
      "                                        rates.\r\n",
      "  --adax                                use adaptive learning rates with x^2 \r\n",
      "                                        instead of g^2x^2\r\n",
      "  --invariant                           use safe/importance aware updates.\r\n",
      "  --normalized                          use per feature normalized updates\r\n",
      "  --sparse_l2 arg (=0)                  use per feature normalized updates\r\n",
      "  --l1_state arg (=0)                   use per feature normalized updates\r\n",
      "  --l2_state arg (=1)                   use per feature normalized updates\r\n",
      "\r\n",
      "Input options:\r\n",
      "  -d [ --data ] arg                     Example Set\r\n",
      "  --daemon                              persistent daemon mode on port 26542\r\n",
      "  --foreground                          in persistent daemon mode, do not run \r\n",
      "                                        in the background\r\n",
      "  --port arg                            port to listen on; use 0 to pick unused\r\n",
      "                                        port\r\n",
      "  --num_children arg                    number of children for persistent \r\n",
      "                                        daemon mode\r\n",
      "  --pid_file arg                        Write pid file in persistent daemon \r\n",
      "                                        mode\r\n",
      "  --port_file arg                       Write port used in persistent daemon \r\n",
      "                                        mode\r\n",
      "  -c [ --cache ]                        Use a cache.  The default is \r\n",
      "                                        <data>.cache\r\n",
      "  --cache_file arg                      The location(s) of cache_file.\r\n",
      "  --json                                Enable JSON parsing.\r\n",
      "  --dsjson                              Enable Decision Service JSON parsing.\r\n",
      "  -k [ --kill_cache ]                   do not reuse existing cache: create a \r\n",
      "                                        new one always\r\n",
      "  --compressed                          use gzip format whenever possible. If a\r\n",
      "                                        cache file is being created, this \r\n",
      "                                        option creates a compressed cache file.\r\n",
      "                                        A mixture of raw-text & compressed \r\n",
      "                                        inputs are supported with \r\n",
      "                                        autodetection.\r\n",
      "  --no_stdin                            do not default to reading from stdin\r\n"
     ]
    }
   ],
   "source": [
    "!vw --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = ./kaggle/400users/train_part_model.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = ./kaggle/400users/train_part.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      262        1       11\n",
      "1.000000 1.000000            2            2.0       82      262       11\n",
      "1.000000 1.000000            4            4.0      241      262       11\n",
      "1.000000 1.000000            8            8.0      352      262       11\n",
      "1.000000 1.000000           16           16.0      135       16       11\n",
      "1.000000 1.000000           32           32.0       71      112       11\n",
      "0.968750 0.937500           64           64.0      358      231       11\n",
      "0.976562 0.984375          128          128.0      348      346       11\n",
      "0.941406 0.906250          256          256.0      202      202       11\n",
      "0.947266 0.953125          512          512.0       30        1       11\n",
      "0.925781 0.904297         1024         1024.0       36      290       11\n",
      "0.908203 0.890625         2048         2048.0       21      128       11\n",
      "0.880127 0.852051         4096         4096.0       80      229       11\n",
      "0.856323 0.832520         8192         8192.0      307      356       11\n",
      "0.828003 0.799683        16384        16384.0       59      193       11\n",
      "0.795441 0.762878        32768        32768.0      262       30       11\n",
      "0.760468 0.725494        65536        65536.0      171      238       11\n",
      "0.724008 0.724008       131072       131072.0        6        6       11 h\n",
      "0.697339 0.670672       262144       262144.0       12       12       11 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 115160\n",
      "passes used = 3\n",
      "weighted example sum = 345480.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.661352 h\n",
      "total feature number = 3800280\n",
      "CPU times: user 231 ms, sys: 66.3 ms, total: 297 ms\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -d $PATH_TO_DATA/train_part.vw --passes 3 --oaa 400 -c -b 26 -f $PATH_TO_DATA/train_part_model.vw --random_seed 17 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ó–∞–ø–∏—à–∏—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ –≤—ã–±–æ—Ä–∫–µ *valid.vw* –≤ *vw_valid_pred.csv*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = ./kaggle/400users/vw_valid_pred.csv\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ./kaggle/400users/valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        4      188       11\n",
      "1.000000 1.000000            2            2.0      160      220       11\n",
      "0.750000 0.500000            4            4.0      143      143       11\n",
      "0.750000 0.750000            8            8.0      247      247       11\n",
      "0.687500 0.625000           16           16.0      341       30       11\n",
      "0.593750 0.500000           32           32.0      237      237       11\n",
      "0.609375 0.625000           64           64.0      178      178       11\n",
      "0.640625 0.671875          128          128.0      132      228       11\n",
      "0.656250 0.671875          256          256.0       14       14       11\n",
      "0.646484 0.636719          512          512.0      370      370       11\n",
      "0.663086 0.679688         1024         1024.0      189      189       11\n",
      "0.655762 0.648438         2048         2048.0      311      311       11\n",
      "0.657227 0.658691         4096         4096.0      195      318       11\n",
      "0.660156 0.663086         8192         8192.0      171      195       11\n",
      "0.657654 0.655151        16384        16384.0      362       51       11\n",
      "0.655121 0.652588        32768        32768.0      248      248       11\n",
      "\n",
      "finished run\n",
      "number of examples = 54838\n",
      "weighted example sum = 54838.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.654583\n",
      "total feature number = 603218\n",
      "CPU times: user 31.7 ms, sys: 23.7 ms, total: 55.4 ms\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i $PATH_TO_DATA/train_part_model.vw -t -d $PATH_TO_DATA/valid.vw \\\n",
    "-p $PATH_TO_DATA/vw_valid_pred.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–°—á–∏—Ç–∞–π—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑—ã *kaggle_data/vw_valid_pred.csv*  –∏–∑ —Ñ–∞–π–ª–∞ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –¥–æ–ª—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π —á–∞—Å—Ç–∏.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([188, 220, 364, ..., 118, 318, 125])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vw_valid_predictions = np.array(pd.read_csv(os.path.join(PATH_TO_DATA, \"vw_valid_pred.csv\"), header=None)).flatten()\n",
    "vw_valid_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34541741128414605\n"
     ]
    }
   ],
   "source": [
    "vw_valid_acc = accuracy_score(vw_valid_predictions, y_valid_for_vw)\n",
    "print(vw_valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–¢–µ–ø–µ—Ä—å –æ–±—É—á–∏—Ç–µ `SGDClassifier` (3 –ø—Ä–æ—Ö–æ–¥–∞ –ø–æ –≤—ã–±–æ—Ä–∫–µ, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å) –∏ `LogisticRegression` –Ω–∞ 70% —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ ‚Äì `(X_train_part_sparse, y_train_part)`, —Å–¥–µ–ª–∞–π—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ `(X_valid_sparse, y_valid)` –∏ –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –¥–æ–ª–∏ –≤–µ—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –Ω–µ –±—ã—Å—Ç—Ä–æ (—É –º–µ–Ω—è ‚Äì 4 –º–∏–Ω—É—Ç—ã) ‚Äì —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ. –£–∫–∞–∂–∏—Ç–µ –≤–µ–∑–¥–µ `random_state`=17, `n_jobs`=-1. –î–ª—è `SGDClassifier` —Ç–∞–∫–∂–µ —É–∫–∞–∂–∏—Ç–µ `max_iter=3`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /media/igor/Acer/Users/arxei/Anaconda/CourseraDeepLearning/Course6/UsersIdentification\n",
      "plugins: anyio-2.2.0\n",
      "collected 43689 items / 1 error / 10991 deselected / 32697 selected\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "__________ ERROR collecting special/tests/test_precompute_gammainc.py __________\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/pluggy/hooks.py:286: in __call__\n",
      "    return self._hookexec(self, self.get_hookimpls(), kwargs)\n",
      "        args       = ()\n",
      "        kwargs     = {'collector': <Module test_precompute_gammainc.py>, 'name': 'test_g', 'obj': <function test_g at 0x7f3a49914ca0>}\n",
      "        notincall  = set()\n",
      "        self       = <_HookCaller 'pytest_pycollect_makeitem'>\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/pluggy/manager.py:93: in _hookexec\n",
      "    return self._inner_hookexec(hook, methods, kwargs)\n",
      "        hook       = <_HookCaller 'pytest_pycollect_makeitem'>\n",
      "        kwargs     = {'collector': <Module test_precompute_gammainc.py>, 'name': 'test_g', 'obj': <function test_g at 0x7f3a49914ca0>}\n",
      "        methods    = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/igor/anaconda3/lib/python3.8/site-package... plugin=<module 'anyio.pytest_plugin' from '/home/igor/anaconda3/lib/python3.8/site-packages/anyio/pytest_plugin.py'>>]\n",
      "        self       = <_pytest.config.PytestPluginManager object at 0x7f3a631c56a0>\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/pluggy/manager.py:84: in <lambda>\n",
      "    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(\n",
      "        hook       = <_HookCaller 'pytest_pycollect_makeitem'>\n",
      "        kwargs     = {'collector': <Module test_precompute_gammainc.py>, 'name': 'test_g', 'obj': <function test_g at 0x7f3a49914ca0>}\n",
      "        methods    = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/igor/anaconda3/lib/python3.8/site-package... plugin=<module 'anyio.pytest_plugin' from '/home/igor/anaconda3/lib/python3.8/site-packages/anyio/pytest_plugin.py'>>]\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/python.py:249: in pytest_pycollect_makeitem\n",
      "    res = list(collector._genfunctions(name, obj))\n",
      "        collector  = <Module test_precompute_gammainc.py>\n",
      "        name       = 'test_g'\n",
      "        obj        = <function test_g at 0x7f3a49914ca0>\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/python.py:458: in _genfunctions\n",
      "    definition = FunctionDefinition.from_parent(self, name=name, callobj=funcobj)\n",
      "        cls        = None\n",
      "        clscol     = None\n",
      "        fm         = <_pytest.fixtures.FixtureManager object at 0x7f3a63111bb0>\n",
      "        funcobj    = <function test_g at 0x7f3a49914ca0>\n",
      "        module     = <module 'scipy.special.tests.test_precompute_gammainc' from '/home/igor/anaconda3/lib/python3.8/site-packages/scipy/special/tests/test_precompute_gammainc.py'>\n",
      "        modulecol  = <Module test_precompute_gammainc.py>\n",
      "        name       = 'test_g'\n",
      "        self       = <Module test_precompute_gammainc.py>\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/python.py:1619: in from_parent\n",
      "    return super().from_parent(parent=parent, **kw)\n",
      "        __class__  = <class '_pytest.python.Function'>\n",
      "        cls        = <class '_pytest.python.FunctionDefinition'>\n",
      "        kw         = {'callobj': <function test_g at 0x7f3a49914ca0>, 'name': 'test_g'}\n",
      "        parent     = <Module test_precompute_gammainc.py>\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/nodes.py:190: in from_parent\n",
      "    return cls._create(parent=parent, **kw)\n",
      "        cls        = <class '_pytest.python.FunctionDefinition'>\n",
      "        kw         = {'callobj': <function test_g at 0x7f3a49914ca0>, 'name': 'test_g'}\n",
      "        parent     = <Module test_precompute_gammainc.py>\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/nodes.py:94: in _create\n",
      "    return super().__call__(*k, **kw)\n",
      "        __class__  = <class '_pytest.nodes.NodeMeta'>\n",
      "        k          = ()\n",
      "        kw         = {'callobj': <function test_g at 0x7f3a49914ca0>, 'name': 'test_g', 'parent': <Module test_precompute_gammainc.py>}\n",
      "        self       = <class '_pytest.python.FunctionDefinition'>\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/python.py:1609: in __init__\n",
      "    fixtureinfo = self.session._fixturemanager.getfixtureinfo(\n",
      "        __class__  = <class '_pytest.python.Function'>\n",
      "        callobj    = <function test_g at 0x7f3a49914ca0>\n",
      "        callspec   = None\n",
      "        config     = None\n",
      "        fixtureinfo = None\n",
      "        keywords   = None\n",
      "        name       = 'test_g'\n",
      "        originalname = None\n",
      "        parent     = <Module test_precompute_gammainc.py>\n",
      "        self       = <FunctionDefinition test_g>\n",
      "        session    = None\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/fixtures.py:1464: in getfixtureinfo\n",
      "    initialnames, names_closure, arg2fixturedefs = fm.getfixtureclosure(\n",
      "        argnames   = ()\n",
      "        cls        = None\n",
      "        fm         = <_pytest.fixtures.FixtureManager object at 0x7f3a63111bb0>\n",
      "        func       = <function test_g at 0x7f3a49914ca0>\n",
      "        funcargs   = True\n",
      "        initialnames = ()\n",
      "        node       = <FunctionDefinition test_g>\n",
      "        self       = <_pytest.fixtures.FixtureManager object at 0x7f3a63111bb0>\n",
      "        usefixtures = ()\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/fixtures.py:1535: in getfixtureclosure\n",
      "    fixturedefs = self.getfixturedefs(argname, parentid)\n",
      "        arg2fixturedefs = {}\n",
      "        argname    = 'check_fpu_mode'\n",
      "        fixturenames = ()\n",
      "        fixturenames_closure = ['check_fpu_mode']\n",
      "        ignore_args = []\n",
      "        initialnames = ('check_fpu_mode',)\n",
      "        lastlen    = 1\n",
      "        merge      = <function FixtureManager.getfixtureclosure.<locals>.merge at 0x7f3a4a87adc0>\n",
      "        parentid   = 'special/tests/test_precompute_gammainc.py::test_g'\n",
      "        parentnode = <FunctionDefinition test_g>\n",
      "        self       = <_pytest.fixtures.FixtureManager object at 0x7f3a63111bb0>\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/fixtures.py:1672: in getfixturedefs\n",
      "    return tuple(self._matchfactories(fixturedefs, nodeid))\n",
      "        argname    = 'check_fpu_mode'\n",
      "        fixturedefs = [<FixtureDef argname='check_fpu_mode' scope='function' baseid=''>]\n",
      "        nodeid     = 'special/tests/test_precompute_gammainc.py::test_g'\n",
      "        self       = <_pytest.fixtures.FixtureManager object at 0x7f3a63111bb0>\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/fixtures.py:1677: in _matchfactories\n",
      "    parentnodeids = set(nodes.iterparentnodeids(nodeid))\n",
      "        fixturedefs = [<FixtureDef argname='check_fpu_mode' scope='function' baseid=''>]\n",
      "        nodeid     = 'special/tests/test_precompute_gammainc.py::test_g'\n",
      "        self       = <_pytest.fixtures.FixtureManager object at 0x7f3a63111bb0>\n",
      "/home/igor/anaconda3/lib/python3.8/site-packages/_pytest/nodes.py:67: in iterparentnodeids\n",
      "    at = nodeid.find(sep, pos)\n",
      "E   KeyboardInterrupt\n",
      "        nodeid     = 'special/tests/test_precompute_gammainc.py::test_g'\n",
      "        pos        = 0\n",
      "        sep        = '/'\n",
      "=========================== short test summary info ============================\n",
      "ERROR special/tests/test_precompute_gammainc.py - KeyboardInterrupt\n",
      "!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n",
      "====================== 10991 deselected, 1 error in 9.24s ======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "# scipy.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(solver='liblinear', random_state=17, n_jobs=-1)\n",
    "sgd_logit = SGDClassifier(loss='log', max_iter=3, random_state=17, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 7min 52s, sys: 59.5 s, total: 1h 8min 52s\n",
      "Wall time: 17min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(n_jobs=-1, random_state=17, solver='liblinear')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit.fit(X_train_part_sparse, y_train_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36292352018673185\n"
     ]
    }
   ],
   "source": [
    "logit_pred = logit.predict(X_valid_sparse)\n",
    "logit_valid_acc = accuracy_score(logit_pred, y_valid)\n",
    "print(logit_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 43.7 s, total: 2min 16s\n",
      "Wall time: 17.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log', max_iter=3, n_jobs=-1, random_state=17)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit.fit(X_train_part_sparse, y_train_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2936467413107699\n"
     ]
    }
   ],
   "source": [
    "sgd_logit_pred = sgd_logit.predict(X_valid_sparse)\n",
    "sgd_logit_valid_acc = accuracy_score(sgd_logit_pred, y_valid)\n",
    "print(sgd_logit_valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>–í–æ–ø—Ä–æ—Å 1. </font> –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –¥–æ–ª—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–ª—è Vowpal Wabbit, –æ–∫—Ä—É–≥–ª–∏—Ç–µ –¥–æ 3 –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π.**\n",
    "\n",
    "**<font color='red'>–í–æ–ø—Ä–æ—Å 2. </font> –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –¥–æ–ª—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–ª—è SGD, –æ–∫—Ä—É–≥–ª–∏—Ç–µ –¥–æ 3 –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π.**\n",
    "\n",
    "**<font color='red'>–í–æ–ø—Ä–æ—Å 3. </font> –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –¥–æ–ª—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –æ–∫—Ä—É–≥–ª–∏—Ç–µ –¥–æ 3 –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw_valid_acc = round(vw_valid_acc, 3)\n",
    "sgd_valid_acc = round(sgd_logit_valid_acc, 3)\n",
    "logit_valid_acc = round(logit_valid_acc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer_to_file(answer, file_address):\n",
    "    with open(file_address, 'w') as out_f:\n",
    "        out_f.write(str(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer_to_file(round(vw_valid_acc, 3), 'Answers/answer6_1.txt')\n",
    "write_answer_to_file(round(sgd_valid_acc, 3), 'Answers/answer6_2.txt')\n",
    "write_answer_to_file(round(logit_valid_acc, 3), 'Answers/answer6_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ (Public Leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å VW —Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–∞ –≤—Å–µ–π –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ ‚Äì *train.vw*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = ./kaggle/400users/train_model.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ./kaggle/400users/train.vw.cache\n",
      "Reading datafile = ./kaggle/400users/train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      262        1       11\n",
      "1.000000 1.000000            2            2.0       82      262       11\n",
      "1.000000 1.000000            4            4.0      241      262       11\n",
      "1.000000 1.000000            8            8.0      352      262       11\n",
      "1.000000 1.000000           16           16.0      135       16       11\n",
      "1.000000 1.000000           32           32.0       71      112       11\n",
      "0.968750 0.937500           64           64.0      358      231       11\n",
      "0.976562 0.984375          128          128.0      348      346       11\n",
      "0.941406 0.906250          256          256.0      202      202       11\n",
      "0.947266 0.953125          512          512.0       30        1       11\n",
      "0.925781 0.904297         1024         1024.0       36      290       11\n",
      "0.908203 0.890625         2048         2048.0       21      128       11\n",
      "0.880127 0.852051         4096         4096.0       80      229       11\n",
      "0.856323 0.832520         8192         8192.0      307      356       11\n",
      "0.828003 0.799683        16384        16384.0       59      193       11\n",
      "0.795441 0.762878        32768        32768.0      262       30       11\n",
      "0.760468 0.725494        65536        65536.0      171      238       11\n",
      "0.725319 0.690170       131072       131072.0      180      159       11\n",
      "0.692989 0.692989       262144       262144.0       88      221       11 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 164514\n",
      "passes used = 3\n",
      "weighted example sum = 493542.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.642595 h\n",
      "total feature number = 5428962\n",
      "CPU times: user 321 ms, sys: 59.6 ms, total: 380 ms\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -d $PATH_TO_DATA/train.vw --passes 3 --oaa 400 -c -k -b 26 -f $PATH_TO_DATA/train_model.vw --random_seed 17 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–°–¥–µ–ª–∞–π—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = ./kaggle/400users/test_pred.csv\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ./kaggle/400users/test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        1       90       11\n",
      "1.000000 1.000000            2            2.0        1       21       11\n",
      "1.000000 1.000000            4            4.0        1      265       11\n",
      "1.000000 1.000000            8            8.0        1      137       11\n",
      "1.000000 1.000000           16           16.0        1      273       11\n",
      "1.000000 1.000000           32           32.0        1      384       11\n",
      "1.000000 1.000000           64           64.0        1      139       11\n",
      "1.000000 1.000000          128          128.0        1       85       11\n",
      "1.000000 1.000000          256          256.0        1       25       11\n",
      "0.994141 0.988281          512          512.0        1      364       11\n",
      "0.990234 0.986328         1024         1024.0        1      202       11\n",
      "0.992188 0.994141         2048         2048.0        1      181       11\n",
      "0.993652 0.995117         4096         4096.0        1       21       11\n",
      "0.994629 0.995605         8192         8192.0        1      137       11\n",
      "0.995300 0.995972        16384        16384.0        1      326       11\n",
      "0.994568 0.993835        32768        32768.0        1       10       11\n",
      "\n",
      "finished run\n",
      "number of examples = 46473\n",
      "weighted example sum = 46473.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.994642\n",
      "total feature number = 511203\n",
      "CPU times: user 34.2 ms, sys: 18.9 ms, total: 53.1 ms\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i $PATH_TO_DATA/train_model.vw -t -d $PATH_TO_DATA/test.vw \\\n",
    "-p $PATH_TO_DATA/test_pred.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ó–∞–ø–∏—à–∏—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑ –≤ —Ñ–∞–π–ª, –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –æ–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ (–±—ã–ª LabelEncoder –∏ –ø–æ—Ç–æ–º +1 –≤ –º–µ—Ç–∫–∞–º) –∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ Kaggle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265    1566\n",
       "87     1507\n",
       "287    1251\n",
       "358    1000\n",
       "144     961\n",
       "       ... \n",
       "19        2\n",
       "50        2\n",
       "93        2\n",
       "104       1\n",
       "335       1\n",
       "Length: 393, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vw_pred = pd.read_csv(os.path.join(PATH_TO_DATA, \"test_pred.csv\"), header=None)\n",
    "vw_pred.head()\n",
    "vw_pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224  48 795 ... 107 387 179]\n"
     ]
    }
   ],
   "source": [
    "vw_pred_transformed = class_encoder.inverse_transform(vw_pred - 1)\n",
    "print(vw_pred_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(vw_pred_transformed, os.path.join(PATH_TO_DATA, 'vw_400_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id  user_id\n",
       "0           1      224\n",
       "1           2       48\n",
       "2           3      795\n",
       "3           4      656\n",
       "4           5      946"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testt = pd.read_csv(os.path.join(PATH_TO_DATA, 'vw_400_users.csv'))\n",
    "testt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–°–¥–µ–ª–∞–π—Ç–µ —Ç–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è SGD –∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏. –¢—É—Ç —É–∂–µ –∂–¥–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å–æ–≤—Å–µ–º —Å–∫—É—á–Ω–æ (–∑–∞–Ω–æ–≤–æ –∑–∞–ø—É—Å–∫–∞—Ç—å —Ç–µ—Ç—Ä–∞–¥–∫—É –≤–∞–º –Ω–µ –∑–∞—Ö–æ—á–µ—Ç—Å—è), –Ω–æ –¥–∞–≤–∞–π—Ç–µ –¥–æ–∂–¥–µ–º—Å—è.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(solver='liblinear', random_state=17, n_jobs=-1)\n",
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 53min 30s, sys: 1min 37s, total: 1h 55min 8s\n",
      "Wall time: 28min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit.fit(X_train_sparse, y)\n",
    "logit_pred = logit.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(logit_pred, os.path.join(PATH_TO_DATA, \"logit_pred.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 23s, sys: 48.9 s, total: 5min 12s\n",
      "Wall time: 40.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit.fit(X_train_sparse, y)\n",
    "sgd_logit_pred = sgd_logit.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(sgd_logit_pred, os.path.join(PATH_TO_DATA, \"sgd_pred.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –¥–æ–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø—É–±–ª–∏—á–Ω–æ–π —á–∞—Å—Ç–∏ (public leaderboard) —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ [—ç—Ç–æ–≥–æ](https://kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "**<font color='red'>–í–æ–ø—Ä–æ—Å 4. </font> –ö–∞–∫–æ–≤–∞ –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø—É–±–ª–∏—á–Ω–æ–π —á–∞—Å—Ç–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (public leaderboard)  –¥–ª—è Vowpal Wabbit?**\n",
    "\n",
    "**<font color='red'>–í–æ–ø—Ä–æ—Å 5. </font> –ö–∞–∫–æ–≤–∞ –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø—É–±–ª–∏—á–Ω–æ–π —á–∞—Å—Ç–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (public leaderboard)  –¥–ª—è SGD?**\n",
    "\n",
    "**<font color='red'>–í–æ–ø—Ä–æ—Å 6. </font> –ö–∞–∫–æ–≤–∞ –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø—É–±–ª–∏—á–Ω–æ–π —á–∞—Å—Ç–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (public leaderboard)  –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw_lb_score, sgd_lb_score, logit_lb_score = 0.18768, 0.18316, 0.19887\n",
    "\n",
    "write_answer_to_file(round(vw_lb_score, 3), 'Answers/answer6_4.txt')\n",
    "write_answer_to_file(round(sgd_lb_score, 3), 'Answers/answer6_5.txt')\n",
    "write_answer_to_file(round(logit_lb_score, 3), 'Answers/answer6_6.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –ø–æ –∑–∞–¥–∞–Ω–∏—é:**\n",
    "- –ü—Ä–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è VW, SGD –∏ logit –≤—ã–≤–æ–¥—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å–¥–µ–ª–∞—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ\n",
    "- –ü–æ–∂–∞–ª—É–π, –∑–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ 400 –∫–ª–∞—Å—Å–æ–≤ (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è 400 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π) —Ä–µ—à–∞–µ—Ç—Å—è –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ö–æ—Ä–æ—à–æ –ø—Ä–∏ —á–µ—Å—Ç–Ω–æ–º –æ—Ç–¥–µ–ª–µ–Ω–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ –æ—Ç –æ–±—É—á–∞—é—â–µ–π. –î–∞–ª–µ–µ –º—ã –±—É–¥–µ–º —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç—å—Å—è –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≠–ª–∏—Å) ‚Äì [–≤–æ—Ç](https://kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ, –≤ –∫–æ—Ç–æ—Ä–æ–º –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–æ—É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å. –ù–µ –ø–µ—Ä–µ–ø—É—Ç–∞–π—Ç–µ! \n",
    "\n",
    "**–£–¥–∞—á–∏!**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
